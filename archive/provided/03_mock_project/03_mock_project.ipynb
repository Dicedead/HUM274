{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from music21 import scale, pitch, note, stream, chord, instrument, midi, converter\n",
    "import numpy as np\n",
    "import scipy.io.wavfile as wave\n",
    "from IPython.display import Audio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mock Project\n",
    "\n",
    "Welcome!\n",
    "\n",
    "This notebook has\n",
    "* the assignment\n",
    "* the materials you may use\n",
    "* explanations on how you might get to your goal of producing an audio file and a visualization\n",
    "\n",
    "## Assignment\n",
    "\n",
    "* Deliverables:\n",
    "  * audio file (1-3 minutes)\n",
    "  * a visualization of your composition\n",
    "* Deadline: Monday, 29th of March, at noon \n",
    "\n",
    "We ask you to compose a a piece from the given materials. You are free in the number of elements you want to be using. For example, if you don't want to work with the audio samples, or only with the audio samples, you may. The only rule is that you cannot add additional material, just transform what you are given. Otherwise, let your creativity go wild!\n",
    "\n",
    "Differently from your semester project that you will work on in the second part of the semester, here we are not asking you to start off with a particular musical idea. Rather, you select musical elements and develop an (algorithmic?) compositional idea from it. So you might probably want to start by exploring the materials and what you can do to them with techniques we have seen in class or others. Communicate with your team members to see what cool ideas and sounds come up and make a plan on how to combine them. \n",
    "\n",
    "Visualizing your composition *before* you start hacking it together is a very, very good idea because it can guide you in the process. Naturally, the visualization that you sketch in the beginning does not have to be identical with the one that you hand in at the end. You are free to come up with any form of visualization you find effective in conveying how the materials you choose are organized in time. Also, come up with a description of your composition in one sentence and a title. \n",
    "\n",
    "Don't have your creativity constrained by what you already know. Instead, if you have a great idea but don't know how to realize it technically, do your own research and/or post your problem on Piazza so you can get quick advice. \n",
    "\n",
    "## Tools\n",
    "\n",
    "To work with symbolic materials you can use python and music21.\n",
    "\n",
    "To organize sound samples in time, you can use [Earsketch](https://earsketch.gatech.edu/landing/#/).\n",
    "\n",
    "To manipulate raw audio, butcher and transform the sound samples, you can use [Audacity](https://www.audacityteam.org/).\n",
    "\n",
    "Note that the music you produce in music21 can be exported in midi format and eventually [converted](https://www.zamzar.com/convert/midi-to-wav/) in .wav format, so that it becomes effectively a new sound sample that you can manipulate in Audacity and combine with the other samples in Earsketch. \n",
    "\n",
    "## Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_measure(pitches, dur=1, **kwargs):\n",
    "    st = stream.Measure(**kwargs)\n",
    "    for p in pitches:\n",
    "        st.append(note.Note(p, quarterLength=dur))\n",
    "    return st\n",
    "\n",
    "def play(stream):\n",
    "    \"\"\"Shortcut to play a stream\"\"\"\n",
    "    midi.realtime.StreamPlayer(stream).play()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Materials: Pitch class collections\n",
    "\n",
    "#### Nice Partials\n",
    "\n",
    "Partials 5, 7, 9, 11, and 13, exemplified over C2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p5 = pitch.Pitch('E4', microtone=-14)\n",
    "p7 = pitch.Pitch('Bb4', microtone=-31)\n",
    "p9 = pitch.Pitch('D5', microtone=4)\n",
    "p11 = pitch.Pitch('F#5', microtone=-49)\n",
    "p13 = pitch.Pitch('A5', microtone=41)\n",
    "partials = [p5, p7, p9, p11, p13]\n",
    "partials_concrete = scale.ConcreteScale(pitches=partials)\n",
    "intervals = partials_concrete.abstract.getIntervals()\n",
    "intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[round(i.semitones, 2) for i in intervals]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Nice scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nice_scale = [pitch.Pitch(p) for p in ('C', 'Db', 'E', 'F#', 'G#', 'A', 'B')]\n",
    "get_measure(nice_scale).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Nice pitch collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nice_collection = [pitch.Pitch(p) for p in ('F', 'Gb', 'A#', 'B')]\n",
    "chord.Chord(nice_collection).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Materials: Samples\n",
    "### Goat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "goat_sample_rate, goat_wav = wave.read('goat.wav')\n",
    "goat_wav[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Audio(goat_wav.T, rate=goat_sample_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Broken Violin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Audio('broken_violin.wav')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pacman"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Audio('pacman.wav')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Underwater"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Audio('underwater.wav')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Audio('chatter.wav')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Materials: Timbres"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Built-in timbres: accordion, violin, clarinet, woodblock"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use 4 different timbres that are built-in in music21: accordion, violin, clarinet, woodblock."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "melody = converter.parse('frere_jacques.mid').parts[0]\n",
    "play(melody)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instruments = [instrument.Accordion(), instrument.Violin(), instrument.Clarinet(), instrument.Woodblock()]\n",
    "\n",
    "#Selecting an instrument for a stream\n",
    "melody.insert(0, instruments[2]) #<----\n",
    "play(melody)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have created your melody with the given instrument in music21, you can export the audio as a `.wav` file, to further edit it (e.g., with Audacity) or to use it in combination with the sound samples (e.g. in EarSketch):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "melody.write('midi', fp = 'frere_jaques_newTimbre.mid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus timbre: pure sine waves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On top of using the four timbres (accordion, violin, clarinet and woodblock) directly in music21, you can also play your melodies using pure sine waves. A sine wave is the most elementary type of wave, as all other waves can be obtained by combining sine waves. \n",
    "\n",
    "For our purposes, a sine wave is defined simply as a vector of the quantized values of a sine function with given frequency and duration. The quantization is parametrized by the framerate, the number of values retained for every second. The vector is stored as a `numpy` array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FRAMERATE = 44100 # <- rate of sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sine(duration, freq = 440, framerate = FRAMERATE):\n",
    "    \"\"\"Returns a sine wave with given frequency (in Hz) and duration (in s), expressed as a vector of quantized values\"\"\"\n",
    "    t = np.linspace(0, duration, int(framerate * duration)) # <- setup time values\n",
    "    wave = np.sin(2 * np.pi * freq * t)\n",
    "    \n",
    "    return wave"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, here is 1 second of a sine wave with frequency 420 Hz:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sine(1, 420)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the wave is exactly 1s long, the length of the vector is exactly the framerate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sine(1, 420))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can play the wave with the following function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_sound(wave, framerate = FRAMERATE):\n",
    "    #Plays a sound wave expressed as an array\n",
    "    return Audio(wave, rate = framerate, autoplay=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "play_sound(sine(1, 420))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can easily create sequences of sine waves by concatenating several arrays. We might also want to include some rests (silences) here and there:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def silence(duration, framerate = FRAMERATE):\n",
    "    #Returns array of silence\n",
    "    t = np.linspace(0, duration, int(framerate * duration))\n",
    "    \n",
    "    return 0*t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Just as an example, a sequence of 1s@420Hz, 2s@220Hz, then 1s of silence, and finally two short 0.5s sounds @330 and 420 Hz.\n",
    "sequence_of_sounds = np.concatenate([sine(1, 420), sine(2, 220), silence(1), sine(0.5, 330), sine(0.5, 420)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "play_sound(sequence_of_sounds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have generated melodies or other pitch materials using the full power of python and music21, we can then transform it into a sequence of sine waves (and silences) with the help of the following function: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def notes_to_sines(melody):\n",
    "    \"\"\"Transforms a melody (as a music21 stream) into a sequence of sine waves\"\"\"\n",
    "    \n",
    "    try: #try to extract tempo\n",
    "        tempo =  melody.duration.quarterLength / melody.seconds #beats per second\n",
    "    except: #if no tempo is specified, default is 120bpm\n",
    "        tempo = 2 #beats per second\n",
    "        \n",
    "    \n",
    "    notes = melody.flat.notesAndRests\n",
    "    \n",
    "    sines = []\n",
    "    for x in notes:\n",
    "        if x.isRest:\n",
    "            sines += [silence(x.quarterLength / tempo)]\n",
    "        else:\n",
    "            sines += [sine(x.quarterLength / tempo, x.pitch.frequency)]\n",
    "    \n",
    "    return np.concatenate(sines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "play_sound(notes_to_sines(melody))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Earsketch for placing audio samples on a temporal grid\n",
    "\n",
    "**Given:** a list of up to 10 samples and a trigger map mapping an onset to one of the samples. Every onset stops the playback of a previous sample that might still be playing (several layers can be created that way if you want samples to overlap)\n",
    "\n",
    "**Goal:** an audio file that contains the samples at the desired positions.\n",
    "\n",
    "This process can be used to plug together your entire composition, or for producing new samples that can then be used in the next iteration.\n",
    "\n",
    "### 1. Upload samples to Earsketch\n",
    "\n",
    "* Head to https://earsketch.gatech.edu/earsketch2/\n",
    "* In order to upload, you need to create an account.\n",
    "* On the left side, open the \"Sounds\" tab and click \"Add sound\". You should see the upload mask:\n",
    "\n",
    "<img src=\"earsketch_upload.png\">\n",
    "\n",
    "* under \"Constant Value\" add a meaningful name. Note that it will be capitalized and your username will be prepended.\n",
    "* Scroll down in the sound collection until you see the category with your username. Here you find the names of all your uploaded samples which you can use as variables in the Python code on the right.\n",
    "\n",
    "### 2. Combine samples and triggermap\n",
    "\n",
    "<img src=\"earsketch_playback.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* In the code field, the first three lines and the last one always need to be there. We set the tempo to 60 bpm, so that every beat has the duration of 1 second.\n",
    "* Then we define the list with the `samples` that will be triggered and note the list index of each.\n",
    "* Now we define a triggermap `onsets`:\n",
    "  * every position in the string corresponds to 1/4 of a beat, so 0.25 seconds is the lowest resolution (in this tempo)\n",
    "  * an integer 0-9 causes the sample at this index to be played back\n",
    "  * every `+` causes the playback to continue, every `-` stands for a rest\n",
    "* then we combine the `samples` list with the `onsets`, using the function `makeBeat()`. The arguments are:\n",
    "  * a single sample or list of samples\n",
    "  * the track (layer) in which the samples are to be played\n",
    "  * the measure in which to begin the playback\n",
    "  * the triggerlist\n",
    "* once defined, we can hit the green \"Run\" button and see the outcome on top in the Digital Audio Workstation (DAW)\n",
    "* in the example, you see that\n",
    "  * the triggermap `onsets` contains 1 for the second sample, `JEYES_GOAT` and 0 for the first sample, `JEYES_HARM`\n",
    "  * it is put into the first track at measure 2\n",
    "  * the second call to the `makeBeat` function only receives one sample and therefore only uses 0 in the triggermap, which also contains rests, i.e, for `0-`, only the first 0.25 seconds of the sample are being played.\n",
    "  \n",
    "Here are a few lines of code to give you an idea how you can translate your durations expressed in seconds to such a triggermap:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def duration2earsketch(dur, tempo=60):\n",
    "    \"\"\"For a duration in seconds and a tempo, return the number of symbols to put into the trigger map.\"\"\"\n",
    "    resolution = 4 * round(tempo) / 60\n",
    "    return round(float(dur) * resolution)\n",
    "\n",
    "def earsketch_event(sample_no, positions, sounding_positions=None):\n",
    "    \"\"\"Pass the index of the sample to be played, the number you got from duration2earsketch,\n",
    "    and how many of the positions you want the sample to keep sounding (the rest is filled up with rests).\"\"\"\n",
    "    if sample_no is None:\n",
    "        return '-' * positions\n",
    "    positions -= 1\n",
    "    sounding_positions = positions if sounding_positions is None else sounding_positions - 1\n",
    "    rest_positions = positions - sounding_positions\n",
    "    return f\"{sample_no}{'+' * sounding_positions}{'-' * rest_positions}\"\n",
    "\n",
    "def durations2triggermap(succession_of_samples, onsets, tempo=60):\n",
    "    if isinstance(succession_of_samples, int):\n",
    "        succession_of_samples = [succession_of_samples]\n",
    "    nxt_sample = (s for s in succession_of_samples)\n",
    "    result = ''\n",
    "    for ons in onsets:\n",
    "        positions = duration2earsketch(ons)\n",
    "        if isinstance(ons, str):\n",
    "            result += earsketch_event(None, positions)\n",
    "        else:\n",
    "            try:\n",
    "                sample_no = next(nxt_sample)\n",
    "            except:\n",
    "                break\n",
    "            result += earsketch_event(sample_no, positions)\n",
    "    return result\n",
    "            \n",
    "sample_succession = [0, 1, 0]\n",
    "durations_in_seconds = [0, '1', 1.75, 6.9]    \n",
    "durations2triggermap(sample_succession, durations_in_seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Downloading audio from earsketch\n",
    "\n",
    "Just go to the \"Scripts\" tab, find the script where you entered the code and open the menu to select \"Download\".\n",
    "\n",
    "<img src=\"earsketch_download.png\" width=\"300px\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
